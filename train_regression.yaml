# llamafactory-cli train train_regression.yaml > regression.log 2>&1
gradient_checkpointing: true
# --- 1. 核心模型与路径配置 ---
model_name_or_path: Qwen/Qwen2.5-3B-Instruct
output_dir: ./outputs/qwen2.5-3b-regression-lora

# --- 2. 数据集配置 ---
dataset: alpaca_gpt4_data_processed
dataset_dir: ./datasets
# template: regression
cutoff_len: 512
val_size: 0.1

# --- 3. 微调方法与阶段配置 ---
stage: sft
do_train: true
finetuning_type: lora

# --- 4. LoRA 参数配置 ---
lora_target: "q_proj,v_proj"
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.1

# --- 5. 训练超参数配置 ---
per_device_train_batch_size: 8 
per_device_eval_batch_size: 4
gradient_accumulation_steps: 1
ddp_find_unused_parameters: true 
# 学习率与优化器
learning_rate: 1e-4
lr_scheduler_type: "linear"
warmup_ratio: 0.1
weight_decay: 0.01

# 训练流程
num_train_epochs: 10
max_grad_norm: 1.0
# 日志、评估与保存
logging_steps: 1
eval_steps: 10
save_steps: 10
eval_strategy: "steps"
save_strategy: "steps"
do_eval: true
load_best_model_at_end: true
metric_for_best_model: "mse"
greater_is_better: false
# 性能与精度
fp16: false
bf16: true

# --- 6. 激活我们的自定义回归工作流 ---
training_as_regression: true
save_total_limit: 3
early_stopping_patience: 3
early_stopping_threshold: 0.01

# --- 7. 实验跟踪 ---
use_swanlab: true
swanlab_project: "Qwen2.5-SFT-Regression"
swanlab_run_name: "LLaMA Factory-mse-linear"
